package DecisionTree;
import java.io.*;
import java.util.Enumeration;

import weka.core.Attribute;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.NoSupportForMissingValuesException;
import weka.core.Utils;
import Utility.Utility;
import java.util.Random;

public class DecisionTree {
    //The node's successors.
    private DecisionTree[] m_Successors;
    //Attribute used for splitting.
    private Attribute m_Attribute;
    //Class value if node is leaf.
    private double m_ClassValue;
    //Class distribution if node is leaf.
    private double[] m_Distribution;
    // Class attribute of data set.
    private Attribute m_ClassAttribute;
    
    public DecisionTree() {
    }
    
    public void decisionTree() throws Exception {
        BufferedReader file = Utility.readFile("/Users/HaoWu/Documents/Code/CS249_17S_Sun/HW1_Programming/DataMiningAlgorithms/data/decision_tree/house.txt");
        Instances data = new Instances(file);
        int cIdx=data.numAttributes()-1;// The data has four attributes
        Random rand=new Random();
        int folds=5;
        Instances randData = new Instances(data);
        randData.randomize(rand); 
        
        /*for (int n = 0; n < folds; n++) {
        	   Instances train = randData.trainCV(folds, n);
        	   Instances test = randData.testCV(folds, n);
        	   train.setClassIndex(cIdx);
        	   test.setClassIndex(cIdx);
        	   buildClassifier(train);
        	   
        	   // further processing, classification, etc.
        }*/
        
        data.setClassIndex(cIdx);// set the attributes
        buildClassifier(data);
        printOutput(data);//predict 
    }   

    public void buildClassifier(Instances data) throws Exception {
        data = new Instances(data);
        this.makeTree(data);
    }   
    
    private void makeTree(Instances data) throws Exception {
    	if(data.numInstances() == 0) {
    		this.m_Attribute = null;
    		this.m_ClassValue = Instance.missingValue();
    		this.m_Distribution = new double[data.numClasses()];
    	} else {
    		double[] infoGains = new double[data.numAttributes()];// infoGains is the array to store infoGain of different attributes

    		Attribute splitData;
    		//This step compute the infoGain of different attribute
    		
    		for(Enumeration attEnum = data.enumerateAttributes(); attEnum.hasMoreElements(); infoGains[splitData.index()] = this.computeInfoGain(data, splitData)) {
    			splitData = (Attribute)attEnum.nextElement();//Returns the next element of this enumeration if this enumeration object has at least one more element to provide.
    		}

    		this.m_Attribute = data.attribute(Utils.maxIndex(infoGains));// ? set the current attribute to the majority
    		if(Utils.eq(infoGains[this.m_Attribute.index()], 0.0D)) {// if the maximum information gain is zero
    			this.m_Attribute = null;
    			this.m_Distribution = new double[data.numClasses()];

    			Instance j;
    			for(Enumeration var6 = data.enumerateInstances(); var6.hasMoreElements(); ++this.m_Distribution[(int)j.classValue()]) {
    				j = (Instance)var6.nextElement();
    			}

    			Utils.normalize(this.m_Distribution);
    			this.m_ClassValue = (double)Utils.maxIndex(this.m_Distribution);
    			this.m_ClassAttribute = data.classAttribute();
    		} 
    		else {
    			Instances[] var7 = this.splitData(data, this.m_Attribute);
    			this.m_Successors = new DecisionTree[this.m_Attribute.numValues()];

    			for(int var8 = 0; var8 < this.m_Attribute.numValues(); ++var8) {
    				this.m_Successors[var8] = new DecisionTree();
    				this.m_Successors[var8].makeTree(var7[var8]);
    			}
    		}
    	}
    }
    
    private double computeInfoGain(Instances data, Attribute att) throws Exception {
        double infoGain = this.computeEntropy(data);
        Instances[] splitData = this.splitData(data, att);// Split the data according the attribute

        /****************Please Fill Missing Lines Here*****************/
        for (int j = 0; j < att.numValues(); j++) {  
            if (splitData[j].numInstances() > 0) {  
              infoGain -= ((double) splitData[j].numInstances() /  (double) data.numInstances()) * computeEntropy(splitData[j]);  
            }  
        }        
        return infoGain;
    }    
    
    private double computeEntropy(Instances data) throws Exception {
        double[] classCounts = new double[data.numClasses()];// this is class! not attribute
        Instance instance;
        //obtain classCounts of different class
        for(Enumeration instEnum = data.enumerateInstances(); instEnum.hasMoreElements(); ++classCounts[(int)instance.classValue()]) {
            instance = (Instance)instEnum.nextElement();
        }

        double totalEntropy = 0.0D;
        int classNum = data.numClasses();
        double [] classProbVec = new double[classNum];// classProbVec has the percentage pi of each class
        
        for(int j = 0; j < classNum; ++j) {
            if(classCounts[j] > 0.0D) {
                classProbVec[j]= classCounts[j]/data.numInstances();
            }
            else
            	classProbVec[j]=0;
        }
        /****************Please Fill Missing Lines Here*****************/
        
        for(int j=0; j<classNum;j++){
        	if(classProbVec[j]>0){
        		totalEntropy -=classProbVec[j]*Utils.log2(classProbVec[j]);
        	}
        }
        
        return totalEntropy;

    }
    
    private Instances[] splitData(Instances data, Attribute att) {
        Instances[] splitData = new Instances[att.numValues()];// The first one return the number of attribute values (like )

        for(int instEnum = 0; instEnum < att.numValues(); ++instEnum) {// It created multiple arrays for different split, 
        	// each split will have the same spot number
            splitData[instEnum] = new Instances(data, data.numInstances());
        }

        Enumeration var6 = data.enumerateInstances();

        while(var6.hasMoreElements()) {
            Instance i = (Instance)var6.nextElement();
            splitData[(int)i.value(att)].add(i);
        }

        for(int var7 = 0; var7 < splitData.length; ++var7) {
            splitData[var7].compactify();
        }
        return splitData;
    }
    
    //Classifies a given test instance using the decision tree.
    public double classifyInstance(Instance instance) throws NoSupportForMissingValuesException {
        if(instance.hasMissingValue()) {
            throw new NoSupportForMissingValuesException("DecisionTree: no missing values, please.");
        } else {
            return this.m_Attribute == null?this.m_ClassValue:this.m_Successors[(int)instance.value(this.m_Attribute)].classifyInstance(instance);// DFS to find the 
        }
    }
    
    private void printOutput(Instances data) throws IOException, NoSupportForMissingValuesException {
        FileWriter fStream = new FileWriter("/Users/HaoWu/Documents/Code/CS249_17S_Sun/HW1_Programming/DataMiningAlgorithms/data/decision_tree/test_output.txt");     // Output File
        BufferedWriter out = new BufferedWriter(fStream);

        for(int index =0; index<data.numInstances();index++) {
            Instance testRowInstance = data.instance(index);
            double prediction =classifyInstance(testRowInstance);
            double true_label=testRowInstance.classValue();
            System.out.println("yay");
            System.out.println(data.classAttribute().value((int) true_label));
            System.out.println(data.classAttribute().value((int) prediction));
            out.write(data.classAttribute().value((int) prediction));
            out.newLine();
        }
        out.close();
    }    
}
